---
title: "Titanic Lab"
author: "Aaron C Cochran"
date: "September 12, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mice)
library(pscl)
library(ROCR)
library(caret)

test <- read_csv('data/titanic/test.csv')
train <- read_csv('data/titanic/train.csv')
results <- read_csv('data/titanic/titanic_results.csv')
```

# Intro

Our dataset for working on the lab exercises is a list of passengers on the Titanic. This is a classic dataset for teaching machine learning and binary classification, and will serve our purposes well here. This lab is derived from work by **Megan L. Risdal** on the machine learning data science site Kaggle.

In this lab, we'll be tackling 3 main areas:

- Feature engineering
- Missing value imputation
- Prediction using logistic regression

This will require 5 packages.

```{r}
library(tidyverse) # basically, use this all of the time
library(mice) # for imputation
library(scales) # for visualization
library(ggthemes) # for visualization
```

## Dataset

We can obtain the data directly from Kaggle. However, for this lab, I've already downloaded the data and saved it in my working directory. The data are in the `data/titanic/` folder. There are two files called `test.csv` and `train.csv` and denote test and training datasets respectively. 

The training set is used to build the predictive model, and contains 1 extra variable: the outcome. The test set is what we'll use to see how well our model works, and does not contain the outcome variable. It's our job to predict this outcome. In this case, it's predicting whether or not the passenger survived the sinking of the HMS Titanic. 

```{r, echo = TRUE}
test <- read_csv('data/titanic/test.csv')
train <- read_csv('data/titanic/train.csv')

# combined dataset of both test and train data
full <- bind_rows(train, test)
```

### Explore the dataset

```{r}
str(full)
```

Some of these variables are intuitively named, others are not. `SibSp` denotes the number of siblings or spouses the passenger has aboard. `Parch` is the number of parents or children the passenger has aboard. For passengers only traveling with a nanny, the `SibSp` and `Parch` variables would both be 0. `Embarked` is the port they embarked from. It is coded as a single letter: C = Cherbourg, Q = Queenstown, S = Southampton. 

## Feature engineering

The **passenger name** variable contains some extra information beyond the passenger's first and last name. Particularly, it contains the title each passenger went by. Additionally, the surname can be used to represent families. Let's create some variables, or as they say, do some **feature engineering.**

This section uses something known as *regular expressions*, or *regex* to parse the text and extract portions to use in the creation of new variables. Regex is not meant to be easy for humans to read, but for machines it is a snap. This makes learning it difficult, and implementing it often involves some trial and error (even after years of R usage). A good resource to test regular expressions is the website [RegExr](www.regexr.com), available at www.regexr.com. 

```{r}
# grab the title from the passenger name using regular expressions (regex)

full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)

# Show the title counts by sex
table(full$Sex, full$Title)


```

There are some columns with really low counts. We should consider combining these into a "Rare Title" category. Then, there are titles that mean the same thing as others: `Mlle` is Mademoiselle, the French equivalent of `Miss`. `Mme` is Madame, which is analogous to `Mrs`. 

```{r}
rare_title <- c("Dona", "Lady", "the Countess",
                "Capt", "Col", "Don", "Dr", "Major",
                "Rev", "Sir", "Jonkheer")

# reassigning french titles and rare titles
full$Title[full$Title == 'Mlle'] <- 'Miss'
full$Title[full$Title == 'Ms'] <- 'Miss'
full$Title[full$Title == 'Mme'] <- 'Mrs'
full$Title[full$Title %in% rare_title] <- 'Rare Title'

# look at the same table again
table(full$Sex, full$Title)


```

Finally, let's grab the passenger's surname from the passenger name field. 

```{r}
# This uses a function from the apply family of functions. 
# We may not have had time to teach this in this short course, but the gist is
# it applies a function, the part after function(x), over an entire object. 
# In this case, it splits the character string for Name and returns only the 
# surname portion. In the tidyverse functions, this is done faster using the 
# purrr package command called map(). For more info, help(package="purrr") and ?map.

full$Surname <- sapply(full$Name, function(x) strsplit(x, split = '[,.]')[[1]][1])
```

We have `r nlevels(factor(full$Surname))` unique surnames. You can include that count in-line in this document by calling the value directly from the dataset. In this case, I used `nlevels(factor(full$Surname))` as a command to ask how many (`nlevels`) unique surnames (`factor(full$Surname)`) were in the full dataset. 


## Do families sink or swim together?

One more feature to engineer here... **family size**. We can create this by adding the number of siblings/spouses (`SibSp`) and the number of parents/children (`Parch`) together. 

```{r}
# Create a family size variable including the passenger in the count
full$Fsize <- full$SibSp + full$Parch + 1

# Create a family variable
full$Family <- paste(full$Surname, full$Fsize, sep="_")
```

What does this family size variable look like? Let's examine the variable in just the training data. 

```{r}
ggplot(full[1:891,], aes(x=Fsize, fill=factor(Survived))) +
  geom_bar(stat='count', position = 'dodge') +
  scale_x_continuous(breaks = c(1:11)) +
  labs(x='Family Size') +
  ggtitle('Survival by Family Size') +
  theme_few()
```

What stands out here? It looks like passengers travelling alone have a significant penalty to survival. Let's call these people 'singletons.'  Then, things change for family sizes of 2, 3, and 4. After that, there is a survival penalty again. There seem to be three distinct typologies of families in terms of their size and survival. Let's take advantage of that and reduce family size into 3 discrete levels. 

```{r}
# discretize family size
full$FsizeD[full$Fsize==1] <- 'singleton'
full$FsizeD[full$Fsize < 5 & full$Fsize > 1] <- 'small'
full$FsizeD[full$Fsize > 4] <- 'large'

# visualize this variable and the survival in a mosaic plot

mosaicplot(table(full$FsizeD, full$Survived),
           main = 'Family Size by Survival', shade = TRUE)
```

## Cabin number and deck

On the Titanic, the cabin numbers were listed as a letter (Deck) and the number (Cabin). Let's create a deck variable here. You'll notice this column is missing a lot of values. We'll have to address that later during the imputation section. For now, we'll just create the deck variable from what we do have. 

```{r}

# how many missing values
length(is.na(full$Cabin))

full$Deck <- factor(sapply(full$Cabin, function(x) strsplit(x, NULL)[[1]][1]))

full$Deck[1:28] # check the first 28 rows to see what we have...



```


# Missing Values and Imputation

There are a number of ways we can handle this. Casewise deletion is a bad idea given how small our dataset is, so we'll try to impute sensible values for the missing ones, or predict them based on the distribution of the values. 

## Sensible value imputation

Let's look at the `embarked` variable. 

```{r}
# check for missing values. 
# This command will return the unique values in the Embarked variable
subset(full$Embarked, !duplicated(full$Embarked))

# there are NAs among the other 3 embarkation points

# who is NA?

full %>% filter(is.na(Embarked)) # is.na is R's way of asking
# if a value in a field is equal to NA (ie, missing)

```

Looks like Passenger 62 and Passenger 830 are missing their embarkation points. Let's try to infer the embark values for each passenger using other data we deem relevant: **passenger class** and **fare**.


```{r}
# Get rid of our missing passenger IDs
embark_fare <- full %>%
  filter(PassengerId != 62 & PassengerId != 830)

# Use ggplot2 to visualize embarkment, passenger class, & median fare
ggplot(embark_fare, aes(x = Embarked, y = Fare, fill = factor(Pclass))) +
  geom_boxplot() +
  geom_hline(aes(yintercept=80), 
    colour='red', linetype='dashed', lwd=2) +
  scale_y_continuous(labels=dollar_format()) +
  theme_few()
```

It looks like $80 was the fare for 1st Class passengers departing from Cherbourg ('C'). We can probably safely replace these NA values with 'C'. 

```{r}
full$Embarked[c(62, 830)] <- 'C'
```

```{r}
# check for PassengerId's where the Fare is NA. 
full$PassengerId[is.na(full$Fare)]

```

Passenger 1044 has a NA fare. Let's see if we can impute the missing value based on the distribution of fare costs for passengers like this one. It looks like he embarked from Southampton ('S') and has a `Pclass` equal to 3, meaning he's a 3rd class passenger. 

```{r}
full[1044,] # visualize the row for this passenger
```

Looking at other 3rd class passengers from Southampton...

```{r}
ggplot(full[full$Pclass == '3' & full$Embarked == 'S', ], 
  aes(x = Fare)) +
  geom_density(fill = '#99d6ff', alpha=0.4) + 
  geom_vline(aes(xintercept=median(Fare, na.rm=T)),
    colour='red', linetype='dashed', lwd=1) +
  scale_x_continuous(labels=dollar_format()) +
  theme_few()


```

It looks like the fare is `r median(subset(full$Fare, full$Pclass == '3' & full$Embarked == 'S'), na.rm=TRUE)`. I determined that value not from the line on the graph, but from the command `median(subset(full$Fare, full$Pclass == '3' & full$Embarked == 'S'), na.rm=TRUE)` which we used in the graph code to make the line. 

From this, we can safely assume that the fare price for passenger 1044 is the median fare for people who departed from the same port and bought tickets in the same class. 

```{r}
# replace missing fare with median fare for class/embarkment

full$Fare[1044] <- median(full[full$Pclass == '3' &
                                 full$Embarked == 'S', ]$Fare,
                          na.rm=TRUE)
```



## Predictive imputation

One area in our data with a lot of missing values is `age`. It has `r sum(is.na(full$Age))` missing values. 

```{r}
# show missing values
sum(is.na(full$Age))
```

To fix this, we're going to use the `mice` package, which stands for "multiple imputation using chained equations." You can get more info by using the command `help(package="mice")`. 

First, we need to change the class of variables from character strings to factors. This is a requirement for working in `mice`.

```{r}
# make variables into factors
factor_vars <- c('PassengerId', 'Pclass', 'Sex',
                 'Embarked', 'Title', 'Surname',
                 'Family', 'FsizeD')

full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))

# set a random seed. This allows us all to use the same randomness and get the same values. 

set.seed(129)

# perform mice imputation

mice_mod <- mice(full[, !names(full) %in%
                        c('PassengerId',
                          'Name',
                          'Ticket',
                          'Cabin',
                          'Family',
                          'Surname',
                          'Survived')],
                 method = 'rf')
```

Then save the output

```{r}
mice_output <- complete(mice_mod)
```

Now let's compare the imputed results with the original distribution of `age` to make sure everything looks okay. 

```{r}
# plot age distributions
ggplot(data=full) +
  geom_histogram(aes(x=Age, y=..density..),
                 binwidth = 5, color = 'black',
                 fill = 'darkgreen') +
  ggtitle('Age: Original Data')

ggplot(data=mice_output) +
  geom_histogram(aes(x=Age, y=..density..),
                 binwidth = 5, color = 'black',
                 fill = 'lightgreen') +
  ggtitle('Age: MICE Output')

```

That looks pretty close, so we'll replace our age vector in the original data with the output from the `mice` model. 

```{r}
# replace age variable from the mice model
full$Age <- mice_output$Age

# show new number of missing (NA) values
sum(is.na(full$Age))
```

# Feature Engineering Part Deux

Now that we have a complete set of ages, we can compute some age-dependent variables: **Child** and **Mother**. A child will simply be someone under 18 years old, and a mother is someone who is both female and over 18, with more than 0 children, and does not have the title 'Miss.' The last criterion is something that is unique to the timing of the dataset. It is much less likely in 1912 that an unmarried woman would be traveling with children of her own than it would be in, say, 2017. 

```{r, warning=FALSE}
# first we'll look at age vs. survival
ggplot(full[1:891,], aes(Age, fill = factor(Survived))) +
  geom_histogram() +
  # sex is being included since we know, a priori, that it is a significant indicator. 
  facet_grid(.~Sex) +
  theme_few()

```

```{r, warning=FALSE}
# create a column child, and indicate whether or not the passenger is a child or adult
full$Child[full$Age < 18] <- 'Child'
full$Child[full$Age >=18] <- 'Adult'

# show counts
table(full$Child, full$Survived)
```

So, it looks like being a child has some influence on survival, but it does not guarantee it. Now let's make the **mother** variable. 

```{r}
full$Mother <- 'Not mother' # first everyone is just assumed to not be a mother
full$Mother[full$Sex == 'female' &
              full$Parch > 0 &
              full$Age > 18 &
              full$Title !='Miss'] <- 'Mother' # if all 4 criteria are TRUE...

# show counts
table(full$Mother, full$Survived)

```

Now let's make those new variables into factors, instead of character strings. 

```{r}
full$Child <- factor(full$Child)
full$Mother <- factor(full$Mother)
```

Let's use the `mice` command `md.pattern()` to see if there is any other missing data we need to be aware of. 

```{r, warning=FALSE}
md.pattern(full)
```

# Building the model

First we need to split our data up into training and test sets. 

```{r}
# split data back into training and test sets
train <- full[1:891,]
test <- full[892:1309,]
```

Now let's build our regression regression model. 

```{r logit-model}
# Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked


fit <- glm(Survived~Age+Pclass+Sex+SibSp+Parch+Fare+Embarked, data=train, family =binomial(link="logit"))

summary(fit)

```

What if we replaced the Sex field with our engineered field, "Title" and re-ran the model? Would we have any improvement in the model?

```{r}
fit2 <- glm(Survived~Age+Pclass+SibSp+Parch+Fare+Embarked+Title, data=train, family =binomial(link="logit"))
summary(fit2)
```

```{r}
anova(fit, test="Chisq")
anova(fit2, test="Chisq")
```

It looks like `Title` outpreformed the more general `Sex` field and reduced our residual deviance (and AIC). 

We can use the `prcl` package to compute McFadden's pseudo-R^2 which is roughly equivalent to R^2 in OLS. 
```{r}
pR2(fit)

pR2(fit2)
```

We could continue tweaking the model but for the purposes of this lab let's accept our `fit2` model and move on to prediction. 

```{r}
fitted.results <- predict(fit2,newdata=subset(test,select=c(3,6,7,8,10,12,13)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)

test2 <- test %>% select(-Survived) %>% mutate(PassengerId = as.numeric(PassengerId)) %>% left_join(results, by="PassengerId")

misClasificError <- mean(fitted.results != test2$Survived)
print(paste('Accuracy',1-misClasificError))

```

```{r}

# caret package
confusionMatrix(data=fitted.results, reference=test2$Survived)

# ROCR package
pr <- prediction(fitted.results, test2$Survived)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```


